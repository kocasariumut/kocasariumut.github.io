<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Umut  KocasarÄ±


</title>
<meta name="description" content="Umut KocasarÄ± Personal Website
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ“–</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">




  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Umut</span>  KocasarÄ±
    </h1>
     <p class="desc"><a href="https://www.tum.de/" target="_blank" rel="noopener noreferrer">Technical University of Munich</a></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="/assets/resized/umut_pp3-480x701.png" srcset="    /assets/resized/umut_pp3-480x701.png 480w,/assets/img/umut_pp3.png 787w">

      
      
    </div>
    

    <div class="clearfix">
      <h2 id="about-me">About Me</h2>

<p>Hi, Iâ€™m Umut.</p>

<p>I am a PhD student in the Visual Computing and Artificial Intelligence Lab, supervised by Prof. Matthias NieÃŸner.</p>

<p>I completed my Bachelorâ€™s in Computer Engineering at BoÄŸaziÃ§i University and my Masterâ€™s in Computer Science at the Technical University of Munich.</p>

<p>My previous work focused on generative modeling and latent space manipulation in GANs.</p>

<p>My research interests include deep learning, computer graphics, and 3D reconstruction.</p>

<p><strong>Key Topics</strong>: 3D Reconstruction, Generative Modeling, Geometric Deep Learning</p>

<p>You could see <a href="https://github.com/user-attachments/files/23826460/umut_cv_latest.pdf" target="_blank" rel="noopener noreferrer">CV here</a>.</p>

    </div>

    

    
      <div class="publications">
  <h2>Publications</h2>
  <ol class="bibliography">
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_g3dst3.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">GCPR</abbr>
</center>
    
  
  </div>

  <div id="meric2024g3dst" class="col-sm-8">
    
      <div class="title">G3DST: Generalizing 3D Style Transfer with Neural Radiance Fields across Scenes and Styles</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                   Adil Meric,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Umut Kocasari,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Matthias NieÃŸner,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Barbara Roessle
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>DAGM German Conference on Pattern Recognition</em>
      
      
        (<b>GCPR</b>)
      
      
      , 2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2408.13508" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://portal.fis.tum.de/en/publications/g3dst-generalizing-3d-style-transfer-withneural-radiance-fields-a" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Neural Radiance Fields (NeRF) have emerged as a powerful tool for creating highly detailed and photorealistic scenes. Existing methods for NeRF-based 3D style transfer need extensive per-scene optimization for single or multiple styles, limiting the applicability and efficiency of 3D style transfer. In this work, we overcome the limitations of existing methods by rendering stylized novel views from a NeRF without the need for per-scene or per-style optimization. To this end, we take advantage of a generalizable NeRF model to facilitate style transfer in 3D, thereby enabling the use of a single learned model across various scenes. By incorporating a hypernetwork into a generalizable NeRF, our approach enables on-the-fly generation of stylized novel views. Moreover, we introduce a novel flow-based multi-view consistency loss to preserve consistency across multiple views. We evaluate our method across various scenes and artistic styles and show its performance in generating high-quality and multi-view consistent stylized images without the need for a scene-specific implicit model. Our findings demonstrate that this approach not only achieves a good visual quality comparable to that of per-scene methods but also significantly enhances efficiency and applicability, marking a notable advancement in the field of 3D style transfer.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_fantastic.jpeg" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">WACV</abbr>
</center>
    
  
  </div>

  <div id="fantasticstyles" class="col-sm-8">
    
      <div class="title">Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                   Enis Simsar,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Umut Kocasari,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Ezgi Gulperi Er,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Pinar Yanardag
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Winter Conference on Applications of Computer Vision</em>
      
      
        (<b>WACV</b>)
      
      
      , 2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2203.08516" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://catlab-team.github.io/fantasticstyles" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The discovery of interpretable directions in the latent spaces of pre-trained GAN models has recently become a popular topic. In particular, StyleGAN2 has enabled various image generation and manipulation tasks due to its rich and disentangled latent spaces. The discovery of such directions is typically done either in a supervised manner, which requires annotated data for each desired manipulation, or in an unsupervised manner, which requires a manual effort to identify the directions. As a result, existing work typically finds only a handful of directions in which controllable edits can be made. In this paper, we attempt to find the most representative and diverse subset of directions in stylespace of StyleGAN2. We formulate the problem as a coverage of stylespace and propose a novel submodular optimization framework that can be solved efficiently with a greedy optimization scheme. We evaluate our framework with qualitative and quantitative experiments and show that our method finds more diverse and relevant channels.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_paintinstyle.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">CVPR Workshop</abbr>
</center>
    
  
  </div>

  <div id="paintinstyle" class="col-sm-8">
    
      <div class="title">PaintInStyle: One-Shot Discovery of Interpretable Directions by Painting</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                   Berkay Doner*,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Elif Sema Balcioglu*,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Merve Rabia Barin*,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Umut Kocasari,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Mert Tiftikci,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Pinar Yanardag
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Computer Vision for Fashion, Art, and Design</em>
      
      
        (<b>CVPR Workshop</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
    
    
    
    
    
    
      <a href="https://catlab-team.github.io/paintinstyle" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The search for interpretable directions in latent spaces of pre-trained Generative Adversarial Networks (GANs) has become a topic of interest. These directions can be utilized to perform semantic manipulations on the GAN generated images. The discovery of such directions is performed either in a supervised way, which requires manual annotation or pre-trained classifiers, or in an unsupervised way, which requires the user to interpret what these directions represent. Our goal in this work is to find meaningful latent space directions that can be used to manipulate images in a one-shot manner where the user provides a simple drawing (such as drawing a beard or painting a red lipstick) using basic image editing tools. Our method then finds a direction that can be applied to any latent vector to perform the desired edit. We demonstrate that our method is able to find several distinct and fine-grained directions in a variety of datasets.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_rankinstyle.jpeg" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">CVPR Workshop</abbr>
</center>
    
  
  </div>

  <div id="rankinstyle" class="col-sm-8">
    
      <div class="title">Rank in Style: A Ranking-based Approach to Find Interpretable Directions</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                   Umut Kocasari,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Kerem Zaman,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Mert Tiftikci,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Enis Simsar,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Pinar Yanardag
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Computer Vision for Fashion, Art, and Design</em>
      
      
        (<b>CVPR Workshop</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
    
    
    
    
    
    
      <a href="https://catlab-team.github.io/rankinstyle" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent work such as StyleCLIP aims to harness the power of CLIP embeddings for controlled manipulations. Although these models are capable of manipulating images based on a text prompt, the success of the manipulation often depends on careful selection of the appropriate text for the desired manipulation. This limitation makes it particularly difficult to perform text-based manipulations in domains where the user lacks expertise, such as fashion. To address this problem, we propose a method for automatically determining the most successful and relevant text-based edits using a pre-trained StyleGAN model. Our approach consists of a novel mechanism that uses CLIP to guide beam-search decoding, and a ranking method that identifies the most relevant and successful edits based on a list of keywords. We also demonstrate the capabilities of our framework in several domains, including fashion.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_cat_stylemc.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">WACV</abbr>
</center>
    
  
  </div>

  <div id="stylemc" class="col-sm-8">
    
      <div class="title">StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                   Umut Kocasari,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Alara Dirik,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Mert Tiftikci,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Pinar Yanardag
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Winter Conference on Applications of Computer Vision</em>
      
      
        (<b>WACV</b>)
      
      
      , 2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2112.08493" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/catlab-team/stylemc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://catlab-team.github.io/stylemc/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Pre-trained GANs have shown great potential for interpretable directions in the latent space. The discovery of such directions is often done in a supervised or self-supervised manner and requires manual annotations which limits their application in practice. On the other hand, unsupervised approaches provide a way to discover interpretable directions without any supervision, but no fine-grained attribute can be discovered. Recent work such as StyleCLIP aims to overcome this limitation by leveraging the power of CLIP, a joint representational model for text and images, for text-driven image manipulation. While promising, these methods take several hours of pre-processing or training time, and require multiple text prompts. In this work, we propose a fast and efficient method for text-guided image generation and manipulation by leveraging the power of StyleGAN2 and CLIP. Our method uses a CLIP-based loss and an identity loss to manipulate images via user-supplied text prompts without changing any of the irrelevant attributes. Unlike previous work, our method requires only 12 seconds of optimization per text prompt and can be used with any pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method with extensive results and comparisons to state-of-the-art approaches.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row" style="border: dotted 2px gray;">
  <div class="col-sm-4 abbr">
  
    <img src="assets/teaser/teaser_creativegan.png" class="teaser img-fluid z-depth-1">
  
  
    
    <center>  <abbr class="badge">NeurIPS Workshop</abbr>
</center>
    
  
  </div>

  <div id="creativegan" class="col-sm-8">
    
      <div class="title">Exploring Latent Dimensions of Crowd-sourced Creativity</div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                   Umut Kocasari*,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Alperen Bag*,
                
              
            
          
        
          
          
          
          

          
            
              
                
                   Efehan Atici,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Pinar Yanardag
                
              
            
          
        
      </div>

    <div class="periodical">
      
        <em>Machine Learning for Creativity and Design</em>
      
      
        (<b>NeurIPS Workshop</b>)
      
      
      , 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
      
    
    
    
    
      
      <a href="https://arxiv.org/abs/2112.06978" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a href="https://github.com/catlab-team/latentcreative" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
    
      <a href="https://catlab-team.github.io/latentcreative/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Project Page</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent research showed that it is possible to find directions in the latent spaces of pre-trained GANs. These directions provide controllable generation and support a wide range of semantic editing operations such as zoom-in or rotation. While existing works focus on discovering directions for semantic image editing, we focus on an abstract property: Creativity. Can we manipulate an image to make it more or less creative? We build our work on the largest AI-based creativity platform Artbreeder where users are able to generate unique images using pre-trained GAN models. We explore the latent dimensions of the images generated on this platform and present a novel framework for manipulating images to make them more creative.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%75%6D%75%74.%64%65%6E%69%7A%6C%69@%68%6F%74%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=uRN-Q_sAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/kocasariumut" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/umut-kocasari" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>












<!-- <a href="https://kocasariumut.github.io/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> -->

      </div>
      <div class="contact-note">You can reach me via email.
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2025 Umut  KocasarÄ±.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
